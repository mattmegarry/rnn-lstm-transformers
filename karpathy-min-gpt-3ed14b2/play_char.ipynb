{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.char_dataset import CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 962 characters, 37 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "# text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "\n",
    "text = text = open('matt_hall_tiny.txt', 'r').read()\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/07/2023 17:23:02 - INFO - mingpt.model -   number of parameters: 4.226560e+05\n"
     ]
    }
   ],
   "source": [
    "num_layers = 2\n",
    "num_heads = 2\n",
    "embed_dim = 128 # Is this embedding dimension or number of embeddings in some way?\n",
    "\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=num_layers, n_head=num_heads, n_embd=embed_dim)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 1: train loss 3.33415. lr 3.237152e-04: 100%|██████████| 2/2 [00:19<00:00,  9.98s/it]\n",
      "epoch 2 iter 1: train loss 3.19113. lr 6.000000e-05: 100%|██████████| 2/2 [00:19<00:00,  9.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x10a7b8680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 132, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    self._close()\n",
      "  File \"/usr/local/Cellar/python@3.11/3.11.4_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ma senet aaatnlo a eres ntel ilollee la  bielotnoii s a e ansela s aleteentnoarli atr mant e th aeaelis  mri isaltaeenes  ena llannenndtrol nennllel sonnoe et lnre ar aaresetrisn inatrisor adsteedna  m nnlrea tral iriethtrthte liedadso otnrmshne s ndenenn eoeaon ite l onltraltlnte  e o oo ee ien tirdiainratao leaea  eleeetn  n ee eselleolane oteait  e s  lnee nesllnrn alnlnoiieeaol  i iolee stlldltriloanad  the etttteala loinai n  a iseloniloo lrittiiit n oiiilae aner mrldeei lo molltndet l tttatesle le il snedel s adr  mnntedo eine nt tth nea sriea a s  noneetti aeotllrnrmsot etoatotostrll  ne t medtlatt inreal eiesl an ne nniodtltt ltteedeairtndror stien leeshnnndelnal en ntene ltiei llo  eon ed inlt ino lntrnoatatent th odadaonrtoao aadlarriito mlraaatototellri  aao  t art onrrt lin ee aai ntitntonthnten  ndldonlo ed  ale ltenie mla lea aen i nalndaleeal irittteesro lne inite tl ttaaradialdsanoneeendoio si eenlinollllttn oetn   m n ildao   ti   nted ialao lnn titni  on   l l nalnttonie lnrt ia  a t tet neti  sa i ln oteae ea sllrtn tt ln thinlaleela sn mdt ie ldsoao ndenrslte n iairro taerdndononiin tiiot mooattet ltan m   eiisoaiedld nta t ad tl sed iioaliettllt nnlreit tia i n od n nleni iatroo  t trothor l t iitenerrllea ae l  srdthate it ne node  tie illtntaisnnlnen t  ti t lorldotrliordiodlesldniilillralnina leeeldsn einrednaeior  lndtilearn lodl neotron ee e  mnealnnnitlori t tetest satetolrde isliteiies stro s ms  i ean  a eietrl s   t ne  sirtthrnd ios eeldleso iir sen trmea n itrttonrlrden satrtrd en ne tindnd li t tt lnaoetnle i s atlna lntes llarad eath ni tthe ee  nt l intei ard tathtnntnaallrleet eednlet nete olr i einnt nds t esnlei l iatttoan snnll mn  nltanitein mlo io lolen etoeore    ne aed noiaaleli  nl o thn arditl io nao ttindeaonrisaoo mliantle elti a lnnlil oitearnn io sritniiiantnrtteaesirnedaildla irlaiinoltoeltl ee es aoalrlnte sh lren o tln thi lreliiio lesr  trteeeltltnii isrllorilae noan t enaeinaoe allinrdthh nnt adt lrnnoea ne mstoira\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"Ma\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
